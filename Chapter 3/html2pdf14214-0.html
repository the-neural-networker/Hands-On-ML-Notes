<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="K Abhiroop Tejomay" />
  <title>Chapter 3: Classfication</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header>
<h1 class="title">Chapter 3: Classfication</h1>
<p class="author">K Abhiroop Tejomay</p>
<p class="date">2019:09:22</p>
</header>
<h1 id="performance-measures">1. Performance Measures</h1>
<h2 id="accuracy">1.1 Accuracy</h2>
<ul>
<li>Accuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with skewed datasets (i.e., when some classes are much more frequent than others).</li>
</ul>
<div class="sourceCode" id="cb1" data-startFrom=""><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">cross_val_score(sgd_clf, X_train, y_train_5, cv<span class="op">=</span><span class="dv">3</span>, scoring<span class="op">=</span><span class="st">&quot;accuracy&quot;</span>)</a></code></pre></div>
<h2 id="confusion-matrix">1.2 Confusion Matrix</h2>
<ul>
<li>The general idea is to count the number of times instances of class A are classified as class B.</li>
<li>To compute the confusion matrix, you first need to have a set of predictions so that they can be compared to the actual targets.</li>
<li>You can get predictions on the training set using the <code>cross_val_predict</code> function:</li>
</ul>
<div class="sourceCode" id="cb2" data-startFrom=""><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_predict</a>
<a class="sourceLine" id="cb2-2" data-line-number="2"></a>
<a class="sourceLine" id="cb2-3" data-line-number="3">y_train_pred <span class="op">=</span> cross_val_predict(sgd_clf, X_train, y_train_5, cv<span class="op">=</span><span class="dv">3</span>)</a></code></pre></div>
<ul>
<li>Use <code>confusion_matrix</code> method to compute the confusion matrix</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</a>
<a class="sourceLine" id="cb3-2" data-line-number="2">confusion_matrix(y_train_5, y_train_pred)</a></code></pre></div>
<ul>
<li>Correctly classified negative instances are called True Negatives (1st row, 1st col).</li>
<li>Incorrectly classified negative instances as positive are called False Positives (1st row, 2nd col).</li>
<li>Incorrectly classified positive instances as negative are called False Negatives (2nd row, 1st col).</li>
<li>Correctly classified positive instances are called True Positives (2nd row, 2nd col).</li>
</ul>
<h2 id="precision">1.3 Precision</h2>
<ul>
<li>Precision is given by:</li>
</ul>

<ul>
<li>TP is the number of true positives, and FP is the number of false positives.</li>
<li>Precision can be thought of as: Of all the instances that the <strong>classifier</strong> predicts as the positive class, how many are actually positive?</li>
<li>Precision is typically used by another metric named Recall, also called Sensitivity or True Positive Rate.</li>
</ul>
<h2 id="recall">1.4 Recall</h2>
<ul>
<li>Recall is given by:</li>
</ul>

<ul>
<li>Recall can be thought of as: Of all the positive instances that are actually present in the <strong>dataset</strong>, how many are predicted as positive by the classfier?</li>
<li>Below is a figure showing the Confusion Matrix with Precision and Recall.</li>
</ul>
<figure>
<img src="./img/confusion_matrix.png" alt="Illustrated Confusion Matrix" /><figcaption>Illustrated Confusion Matrix</figcaption>
</figure>
<div class="sourceCode" id="cb4" data-startFrom=""><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score, recall_score</a>
<a class="sourceLine" id="cb4-2" data-line-number="2"><span class="bu">print</span>(precision_score(y_train_5, y_train_pred))</a>
<a class="sourceLine" id="cb4-3" data-line-number="3"><span class="bu">print</span>(recall_score(y_train_5, y_train_pred))</a></code></pre></div>
<ul>
<li>It is often convenient to combine precision and recall into a single metric called the F1 score, in particular if you need a simple way to compare two classifiers.</li>
<li>The F1 score is the harmonic mean of precision and recall.</li>
<li>Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values.</li>
<li>As a result, the classifier will only get a high F1 score if both recall and precision are high.</li>
</ul>

<ul>
<li>The F1 score favors classifiers that have similar precision and recall. This is not always what you want.</li>
<li>In some contexts you mostly care about precision, and in other contexts you really care about recall.</li>
</ul>
<h2 id="precisionrecall-tradeoff">1.5 Precision/Recall Tradeoff</h2>
<ul>
<li>Increasing precision reduces recall, and vice versa. This is called the precision/recall trade-off.</li>
<li>To understand this trade-off, let’s look at how the <code>SGDClassifier</code> makes its classification decisions. For each instance, it computes a score based on a decision function. If that score is greater than a threshold, it assigns the instance to the positive class; otherwise it assigns it to the negative class. Figure 2 shows a few digits positioned from the lowest score on the left to the highest score on the right. Suppose the decision threshold is positioned at the central arrow (between the two 5s): you will find 4 true positives (actual 5s) on the right of that threshold, and 1 false positive (actually a 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6 actual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). If you raise the threshold (move it to the arrow on the right), the false positive (the 6) becomes a true negative, thereby increasing the precision (up to 100% in this case), but one true positive becomes a false negative, decreasing recall down to 50%. Conversely, lowering the threshold increases recall and reduces precision.</li>
</ul>
<figure>
<img src="./img/tradeoff.png" alt="Precision/Recall Tradeoff" /><figcaption>Precision/Recall Tradeoff</figcaption>
</figure>
<ul>
<li>Scikit-Learn does not let you set the threshold directly, but it does give you access to the decision scores that it uses to make predictions.</li>
<li>Instead of calling the classifier’s predict() method, you can call its decision<sub>function</sub>() method.</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1">y_scores <span class="op">=</span> sgd_clf.decision_function([some_digit])</a>
<a class="sourceLine" id="cb5-2" data-line-number="2">y_scores</a>
<a class="sourceLine" id="cb5-3" data-line-number="3">threshold <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb5-4" data-line-number="4">y_some_digit_pred <span class="op">=</span> (y_scores <span class="op">&gt;</span> threshold)</a></code></pre></div>
<ul>
<li>How do you decide which threshold to use?</li>
<li>First, use the <code>cross_val_predict()</code> function to get the scores of all instances in the training set, but this time specify that you want to return decision scores instead of predictions:</li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1">y_scores <span class="op">=</span> cross_val_predict(sgd_clf, X_train, y_train_5, cv<span class="op">=</span><span class="dv">3</span>,</a>
<a class="sourceLine" id="cb6-2" data-line-number="2">                             method<span class="op">=</span><span class="st">&quot;decision_function&quot;</span>)</a></code></pre></div>
<ul>
<li>With these scores, use the precision<sub>recallcurve</sub>() function to compute precision and recall for all possible thresholds:</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_curve</a>
<a class="sourceLine" id="cb7-2" data-line-number="2"></a>
<a class="sourceLine" id="cb7-3" data-line-number="3">precisions, recalls, thresholds <span class="op">=</span> precision_recall_curve(y_train_5, y_scores)</a></code></pre></div>
<ul>
<li>Finally, use Matplotlib to plot precision and recall as functions of the threshold value</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="kw">def</span> plot_precision_recall_vs_threshold(precisions, recalls, thresholds):</a>
<a class="sourceLine" id="cb8-2" data-line-number="2">    plt.plot(thresholds, precisions[:<span class="op">-</span><span class="dv">1</span>], <span class="st">&quot;b--&quot;</span>, label<span class="op">=</span><span class="st">&quot;Precision&quot;</span>)</a>
<a class="sourceLine" id="cb8-3" data-line-number="3">    plt.plot(thresholds, recalls[:<span class="op">-</span><span class="dv">1</span>], <span class="st">&quot;g-&quot;</span>, label<span class="op">=</span><span class="st">&quot;Recall&quot;</span>)</a>
<a class="sourceLine" id="cb8-4" data-line-number="4">    [...] <span class="co"># highlight the threshold and add the legend, axis label, and grid</span></a>
<a class="sourceLine" id="cb8-5" data-line-number="5"></a>
<a class="sourceLine" id="cb8-6" data-line-number="6">plot_precision_recall_vs_threshold(precisions, recalls, thresholds)</a>
<a class="sourceLine" id="cb8-7" data-line-number="7">plt.show()</a></code></pre></div>
<figure>
<img src="./img/precision_recall_threshold.png" alt="Precision and Recall versus Decision Threshold" /><figcaption>Precision and Recall versus Decision Threshold</figcaption>
</figure>
<ul>
<li>Precision may sometimes go down when you raise the threshold (although in general it will go up).</li>
<li>On the other hand, recall can only go down when the threshold is increased.</li>
<li>Another way to select a good precision/recall trade-off is to plot precision directly against recall.</li>
</ul>
<figure>
<img src="./img/precision_vs_recall.png" alt="Precision Vs Recall" /><figcaption>Precision Vs Recall</figcaption>
</figure>
</body>
</html>
